# Data Science Graduate

#### Technical Skills: Python, SQL, JMP, Mathematica

## Education
					       		
- M.S., Data Science	| Rochester Institute of Technology , N.Y. (_August 2023_ - Present)	 			        		
- B.S., Data Science and Engineering  | Indian Institute of Science Education and Research, Bhopal (_May 2023_)

## Bachelor's Thesis
### TELI: Teacher-Ensemble distillation based learned Label Interpolation

Imagine a neural network based learning strategy that combines the wisdom of multiple teacher models, dynamically adjusts its learning approach, and enriches the model's understanding of data relationships. Enter TELI.

#### Core Concepts:
##### Knowledge Distillation
The model learns not just from labeled data but distills the collective knowledge of a group of teacher models. These teachers, each a specialist in its own right, contribute nuanced insights to the learning process.

##### Meta Learning 
Using meta-learning, the model dynamically tunes its mixing ratio (lambda) between teacher predictions. This adaptability ensures that the model becomes more than a learner; it becomes an agile decision-maker, adjusting to the complexity of different datasets.

##### Teacher Ensemble
Ensembling is the power of collaboration. TELI brings together an ensemble of teacher models, each providing a unique perspective on the data. The collective intelligence of this ensemble strengthens the model's grasp of intricate patterns.

#### Why Pseudo Mixup?

##### Improved Generalization
TELI doesn't just memorize examples; it learns the relationships between classes. This results in a model that not only performs well on labeled data but excels in making accurate predictions on unseen, unlabeled data.

##### Meta Learning Adaptability 
The ability to dynamically adjust the mixing ratio means the model adapts to the idiosyncrasies of each dataset. No more one-size-fits-all; TELI tailors its approach for optimal performance.

##### Enriched Knowledge Transfer
Knowledge distillation takes center stage, transferring the distilled wisdom of teacher models to the student. The ensemble of teachers ensures a diverse range of knowledge is at the model's disposal.












## Projects
### Data-Driven EEG Band Discovery with Decision Trees
[Publication](https://www.mdpi.com/1424-8220/22/8/3048)


Bachelorâ€™s Thesis : Aiding supervised learning with unlabeled data: Cost-Saving through Semi-Supervised Learning and Knowledge distillation   (Aug/2022 to May/23)

- Developed a novel algorithm which circumvents annotation costs allocated for data collection.
- Utilized heavyweight and lightweight neural networks (e.g., VGG-16, ResNet-50) .
- Performed GradCam analysis to optimize model interpretability, reducing operational errors.
- Improved the robustness of semi-supervised student model with Stochastic Data Augmentation by 2%.
- Utilized custom data-loaders for dynamic data allocation and complete GPU utilisation.
- Boosted the student model performance by 12 % by utilising knowledge distillation.
- Designed an interactive UI for streamlining model training ,testing and evaluation.
0 Presented the research in an annual research exposition and Data science board of my university.



- [Data Science Blog](https://medium.com/@dubemanas10)https://medium.com/@dubemanas10)
