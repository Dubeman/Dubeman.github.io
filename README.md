# Data Science Graduate

#### Technical Skills: Python, SQL, JMP, Mathematica

## Education
					       		
- M.S., Data Science	| Rochester Institute of Technology , N.Y. (_August 2023_ - Present)	 			        		
- B.S., Data Science and Engineering  | Indian Institute of Science Education and Research, Bhopal (_May 2023_)

## Bachelor's Thesis
### TELI: Teacher-Ensemble distillation based learned Label Interpolation

Imagine a neural network based learning strategy that combines the wisdom of multiple teacher models, dynamically adjusts its learning approach, and enriches the model's understanding of data relationships. Enter TELI.

#### Core Concepts:
##### Knowledge Distillation
The model learns not just from labeled data but distills the collective knowledge of a group of teacher models. These teachers, each a specialist in its own right, contribute nuanced insights to the learning process.

##### Meta Learning 
Using meta-learning, the model dynamically tunes its mixing ratio (lambda) between teacher predictions. This adaptability ensures that the model becomes more than a learner; it becomes an agile decision-maker, adjusting to the complexity of different datasets.

##### Teacher Ensemble
Ensembling is the power of collaboration. TELI brings together an ensemble of teacher models, each providing a unique perspective on the data. The collective intelligence of this ensemble strengthens the model's grasp of intricate patterns.

#### Why Pseudo Mixup?

##### Improved Generalization
TELI doesn't just memorize examples; it learns the relationships between classes. This results in a model that not only performs well on labeled data but excels in making accurate predictions on unseen, unlabeled data.

##### Meta Learning Adaptability 
The ability to dynamically adjust the mixing ratio means the model adapts to the idiosyncrasies of each dataset. No more one-size-fits-all; TELI tailors its approach for optimal performance.

##### Enriched Knowledge Transfer
Knowledge distillation takes center stage, transferring the distilled wisdom of teacher models to the student. The ensemble of teachers ensures a diverse range of knowledge is at the model's disposal.

##### Achievements 

- Utilized heavyweight and lightweight neural networks (e.g., VGG-16, ResNet-50, Masked Auto Encoders).
- Performed GradCam analysis to optimize model interpretability.
- Improved the robustness of the semi-supervised student model with Stochastic Data Augmentation by 2%.
- Utilized custom data loaders for dynamic data allocation and complete GPU utilization.
- Boosted the student model performance by 12 % by utilizing knowledge distillation.
- Designed an interactive UI for streamlining model training, testing, and evaluation.
- Presented the research at my university's annual research exposition and Data science board.



- [Data Science Blog](https://medium.com/@dubemanas10)
